{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규 표현식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\nNone\n1234\n"
    }
   ],
   "source": [
    "import re\n",
    "#조건에 맞는 문자가 있음\n",
    "match = re.match('[0-9]', '1234')\n",
    "print(match.group())\n",
    "\n",
    "#조건에 맞는 문자가 없음\n",
    "match = re.match('[0-9]', 'abc')\n",
    "print(match)\n",
    "\n",
    "#0-9까지가 1번이상 반복되는 부분을 찾아옴\n",
    "match = re.match('[0-9]+', '1234')\n",
    "print(match.group()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n<re.Match object; span=(0, 2), match='12'>\n<re.Match object; span=(0, 5), match=' 1234'>\n<re.Match object; span=(1, 5), match='1234'>\n"
    }
   ],
   "source": [
    "import re\n",
    "#앞에 공백이 있어서 찾을 수 없음\n",
    "match = re.match('[0-9]+', ' 1234')\n",
    "print(match)\n",
    "\n",
    "#중간 공백은 상관없음\n",
    "match = re.match('[0-9]+', '12 34')\n",
    "print(match)\n",
    "\n",
    "#앞에 공백문자가 있는 경우는 공백 문자를 의미하는 \\s를 추가해서 조회\n",
    "match = re.match('\\s[0-9]+', ' 1234')\n",
    "print(match)\n",
    "\n",
    "#search 메소드를 이용해서 공백문자가 포함되더라도 찾아오기\n",
    "match = re.search('[0-9]+', ' 1234')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'기상청은 슈퍼컴퓨터도 서울지역의 집중호우를 제대로 예측하지 못했다고 설명했습니다.왜 오류가 발생했는지 자세히 분석해 예측 프로그램을 보완해야 할 대목입니다.머신러닝 알고리즘을 이용하면 조금 더 정확한 예측을 할 수 있지 않을까 기대합니다.박문석 기자'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "string = '''\n",
    "기상청은 슈퍼컴퓨터도 서울지역의 집중호우를 제대로 예측하지 못했다고 설명했습니다.\n",
    "왜 오류가 발생했는지 자세히 분석해 예측 프로그램을 보완해야 할 대목입니다.\n",
    "머신러닝 알고리즘을 이용하면 조금 더 정확한 예측을 할 수 있지 않을까 기대합니다.\n",
    "박문석 기자(ggangpae1@gmail.com)\n",
    "'''\n",
    "\n",
    "import re\n",
    "result = re.sub('\\([a-zA-Z0-9\\._+]+@[a-zA-Z]+\\.(com|org|edu|net|co,kr)\\)', '', string) #이메일 패턴을 찾아서 '' 으로 치환\n",
    "result = re.sub('\\n', '', result) #\\n을 찾아서 ''으로 치환\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영문 대소문자 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hello python\nHELLO PYTHON\n"
    }
   ],
   "source": [
    "string = \"Hello Python\"\n",
    "print(string.lower())\n",
    "print(string.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "★서울의 부동산 가격이 올해들어 % 상승했습니다.!\n 서울의 부동산 가격이 올해들어 상승했습니다 \n"
    }
   ],
   "source": [
    "#숫자에 해당하는 정규식 생성\n",
    "p = re.compile('[0-9+]')\n",
    "string = '★서울의 부동산 가격이 올해들어 10% 상승했습니다.!'\n",
    "#숫자 제거 - string에서 p 패턴에 해당하는 부분을 \"\" 으로 치환\n",
    "result = p.sub(\"\", string)\n",
    "print(result)\n",
    "\n",
    "#특수문자를 제거하기 위해서 특수문자를 정규식으로 생성\n",
    "p = re.compile(\"\\W+\")\n",
    "#단어가 아닌 문자를 \" \"  으로 치환\n",
    "result = p.sub(\" \", result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['연휴', '대이동']\n"
    },
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/jw/nltk_data'\n    - '/Users/jw/opt/anaconda3/nltk_data'\n    - '/Users/jw/opt/anaconda3/share/nltk_data'\n    - '/Users/jw/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/jw/nltk_data'\n    - '/Users/jw/opt/anaconda3/nltk_data'\n    - '/Users/jw/opt/anaconda3/share/nltk_data'\n    - '/Users/jw/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e4ff20653df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#i 가 영문 불용어 사전에 속하지 않은 경우 들만 묶어서 list로 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwords_english\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chief'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'justice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roberts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'presedent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_english\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e4ff20653df0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#i 가 영문 불용어 사전에 속하지 않은 경우 들만 묶어서 list로 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwords_english\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chief'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'justice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roberts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'presedent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_english\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/jw/nltk_data'\n    - '/Users/jw/opt/anaconda3/nltk_data'\n    - '/Users/jw/opt/anaconda3/share/nltk_data'\n    - '/Users/jw/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#한글 불용어 제거\n",
    "words_korean=['추석', '연휴', '민족', '대이동']\n",
    "stopwords=['추석', '민족']\n",
    "\n",
    "#words_korean의 각 요소들을 i에 대입 한 후\n",
    "#i 가 stopwords에 속하지 않은 경우 들만 묶어서 list로 생성\n",
    "r = [i for i in words_korean if i not in stopwords]\n",
    "print(r)\n",
    "\n",
    "#영문 불용어제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#words_english의 각 요소들을 i에 대입 한 후\n",
    "#i 가 영문 불용어 사전에 속하지 않은 경우 들만 묶어서 list로 생성\n",
    "words_english =['chief', 'justice', 'roberts', 'the', 'presedent', 'and']\n",
    "r = [w for w in words_english if  not w in stopwords.words('english')]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 같은 의미의 단어 동일화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = 'All pythoners have pythoned poorly at least once'\n",
    "#영문장의 단어 분할 - 공백을 기준으로 해서 단어들을 분할해서 리스트로 만듬\n",
    "words=word_tokenize(string)\n",
    "print(words)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps_stemmer = PorterStemmer()\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')\n",
    "print()\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer  \n",
    "ls_stemmer=LancasterStemmer()\n",
    "\n",
    "wordlist =[]\n",
    "for w in words:\n",
    "    print(ls_stemmer.stem(w), end=' ')\n",
    "    wordlist.append(ls_stemmer.stem(w))\n",
    "print()\n",
    "\n",
    "r = [w for w in wordlist if  not w in stopwords.words('english')]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "sentence = \"Chief Justice, President Obama, President Obama President Carter\"\n",
    "#연속해서 2번 나온 단어들의 뭉치 만들기\n",
    "grams = ngrams(sentence.split(),2)\n",
    "for gram in grams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(kkma.sentences('한국어의 문장 분석'))\n",
    "print(kkma.nouns('한국어의 단어별 분석'))\n",
    "print(kkma.pos('한국어의 형태소 분석'))\n",
    "\n",
    "print(\"==============================================\")\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "print(hannanum.nouns('한국어의 단어별 분석'))\n",
    "print(hannanum.morphs('한국어의 형태소 분석'))\n",
    "print(hannanum.pos('한국어의 형태소 분석'))\n",
    "\n",
    "print(\"==============================================\")\n",
    "from konlpy.tag import Okt\n",
    "t = Okt()\n",
    "print(t.nouns('한국어의 단어별 분석'))\n",
    "print(t.morphs('한국어의 형태소 분석'))\n",
    "print(t.pos('한국어의 형태소 분석'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영문 품사 분석\n",
    "from nltk import pos_tag\n",
    "#문장을 단어단위로 분할\n",
    "tokens='The little yellow dog barked at the Persian cat'.split()\n",
    "print(tokens)\n",
    "#분할된 단어들의 형태소 분석\n",
    "tags_en=pos_tag(tokens)\n",
    "print(tags_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도 분석 - 워드클라우드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지를 이용한 워드 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "#이미지 출력\n",
    "mask = numpy.array(Image.open('data/appleBar.png'))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자열 생성\n",
    "text = ''\n",
    "for t in range(8):\n",
    "    text = text + 'Python '\n",
    "for t in range(7):\n",
    "    text = text + 'Java '\n",
    "for t in range(7):\n",
    "    text = text + 'C++ '\n",
    "for t in range(8):\n",
    "    text = text + 'JavaScript '\n",
    "for t in range(5):\n",
    "    text = text + 'C# '\n",
    "for t in range(3):\n",
    "    text = text + 'Ruby '\n",
    "for t in range(5):\n",
    "    text = text + 'C '\n",
    "for t in range(2):\n",
    "    text = text + 'scala '\n",
    "for t in range(6):\n",
    "    text = text + 'PHP '\n",
    "for t in range(3):\n",
    "    text = text + 'Swift '\n",
    "for t in range(3):\n",
    "    text = text + 'Kotlin ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제거할 단어 설정\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"Kotlin\")\n",
    "\n",
    "#워드 클라우드 만들기\n",
    "wordcloud = WordCloud(background_color='white', max_words=2000, mask=mask,\n",
    "              stopwords = stopwords)\n",
    "wordcloud = wordcloud.generate(text)\n",
    "wordcloud.words_\n",
    "#워드 클라우드 화면 출력\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대통령 연설문을 이용한 워드클라우드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 내용 읽어오기\n",
    "f = open(\"./data\\\\트럼프취임연설문.txt\", 'r')\n",
    "lines = f.readlines()[0]\n",
    "f.close()\n",
    "\n",
    "lines[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#숫자와 문장부호 등을 제거하기 위한 정규식 생성\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "#영문 불용어 제거를 위한 객체 생성\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#모두 소문자로 변환\n",
    "words =  lines.lower()\n",
    "#정규식 적용\n",
    "tokens = tokenizer.tokenize(words)\n",
    "#불용어 제거\n",
    "stopped_tokens = [i for i in list((tokens)) if not i in stop_words]\n",
    "#글자수가 2자 이상인 것만 모아서 i에 대입\n",
    "stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#단어 개수 세기\n",
    "pd.Series(stopped_tokens2).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/문재인대통령취임연설문.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#한글 형태소 분석기 가져오기\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "#명사만 추출해서 temp list에 추가\n",
    "temp = []\n",
    "for i in range(len(lines)):\n",
    "    temp.append(hannanum.nouns(lines[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp 리스트에서 리스트인 데이터는 분해해서 그렇지 않은 데이터는 바로 word_list에 대입\n",
    "word_list = [] \n",
    "for elem in temp: \n",
    "    if type(elem) == list: \n",
    "        for e in elem: \n",
    "            word_list.append(e) \n",
    "    else: \n",
    "        word_list.append(elem) \n",
    "\n",
    "# 두 글자 이상인 단어만 추출\n",
    "word_list=pd.Series([x for x in word_list if len(x)>1])\n",
    "word_list.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color=\"white\"\n",
    ")\n",
    "\n",
    "#단어 개수 세기\n",
    "count = Counter(stopped_tokens2)\n",
    "\n",
    "#단어 개수를 가지고 워드클라우드를 그릴 데이터 만들기  \n",
    "wordcloud = wordcloud.generate_from_frequencies(count)\n",
    "\n",
    "#워드 클라우드 만들기\n",
    "array = wordcloud.to_array()\n",
    "\n",
    "#워드 클라우드 그리기\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(array, interpolation=\"bilinear\")\n",
    "plt.show()\n",
    "fig.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    font_path = \"c:/Windows/Fonts/malgun.ttf\",\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color=\"white\"\n",
    ")\n",
    "    \n",
    "count = Counter(word_list)\n",
    "\n",
    "wordcloud = wordcloud.generate_from_frequencies(count)\n",
    "\n",
    "array = wordcloud.to_array()\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(array, interpolation=\"bilinear\")\n",
    "plt.show()\n",
    "fig.savefig('wordcloud.png')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "moon_mask=np.array(Image.open(\"./data/appleBar.png\"))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(moon_mask,interpolation=\"bilinear\")\n",
    "plt.show()\n",
    "\n",
    "count = Counter(word_list)\n",
    "wc_moon = WordCloud(\n",
    "    font_path = \"./data/NanumBarunGothic.ttf\",\n",
    "    mask=moon_mask,\n",
    "    background_color=\"white\"\n",
    ")\n",
    "wc_moon = wc_moon.generate_from_frequencies(count)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wc_moon,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신문기사 검색을 이용한 워드클라우드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "keyword = input('검색어:')\n",
    "target_URL = \"http://news.donga.com/search?p=1\" + '&query=' + quote(keyword) + '&check_news=1&more=1&sorting=3&search_date=1&range=3'\n",
    "source_code_from_URL = requests.get(target_URL)\n",
    "bs = BeautifulSoup(source_code_from_URL.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기사 건수를 가져오기 위한 파싱\n",
    "cnt = bs.select(\"div.searchContWrap div.searchCont h2 span\")\n",
    "temp = cnt[0].getText().split(' ')\n",
    "count = int(temp[1])\n",
    "print(\"기사 건수:\",count) \n",
    "page_num = int(input(\"읽어올 기사의 개수:\")) / 15 + 1\n",
    "output_file = open(keyword + \".txt\", 'w', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(page_num)):\n",
    "    current_page_num = 1 + i * 15\n",
    "    target_URL = \"http://news.donga.com/search?p=\" + str(current_page_num) +  '&query=' + quote(\n",
    "        keyword) + '&check_news=1&more=1&sorting=3&search_date=1&range=3'\n",
    "    source_code_from_URL = requests.get(target_URL)\n",
    "    bs = BeautifulSoup(source_code_from_URL.text, 'html.parser')\n",
    "    for title in bs.find_all('p', 'tit'):\n",
    "        title_link = title.select('a')\n",
    "        article_URL = title_link[0]['href']\n",
    "        #print(article_URL)\n",
    "        source_code_from_url = requests.get(article_URL)\n",
    "        soup = BeautifulSoup(source_code_from_url.text, 'html.parser')\n",
    "        content_of_article = soup.select('div.article_txt')\n",
    "        for item in content_of_article:\n",
    "            string_item = str(item.find_all(text=True))\n",
    "            output_file.write(string_item)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "open_text_file = open(keyword + \".txt\", 'r', encoding=\"utf8\")\n",
    "text = open_text_file.read()\n",
    "spliter = Okt()\n",
    "nouns = spliter.nouns(text)\n",
    "open_text_file.close()\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ko = nltk.Text(nouns, name='박문석')\n",
    "print(len(ko.tokens))\n",
    "print(len(set(ko.tokens)))\n",
    "ko.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "    rc('font', family=font_name)\n",
    "plt.figure(figsize=(12,6))\n",
    "ko.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['국민카드', '카드']\n",
    "\n",
    "ko = [each_word for each_word in ko if each_word not in stop_words]\n",
    "ko = [i for i in ko if len(i)>1]\n",
    "print(ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(ko, name='국민카드')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ko.plot(50)     # Plot sorted frequency of top 50 tokens\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "data = ko.vocab().most_common(150)\n",
    "\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.5,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스크 적용\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array(Image.open('data/appleBar.png'))\n",
    "data = ko.vocab().most_common(100)\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.5,\n",
    "                      background_color='white',\n",
    "                      mask=mask,\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장의 유사도 측정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "contents = ['우리 과일 먹으로 가자',\n",
    "                   '나는 고기를 좋아합니다',\n",
    "                   '나는 공원에서 산책하는 것을 싫어합니다',\n",
    "                   '안녕하세요 반갑습니다 그동안 잘 계셨어요']\n",
    "contents_tokens = [Okt.morphs(row) for row in contents]\n",
    "print(contents_tokens)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석 결과 확인\n",
    "contents_for_vectorize = []\n",
    "\n",
    "for content in contents_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "        \n",
    "    contents_for_vectorize.append(sentence)\n",
    "    \n",
    "print(contents_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터의 벡터값 확인\n",
    "X = vectorizer.fit_transform(contents_for_vectorize)\n",
    "num_samples, num_features = X.shape\n",
    "print(num_samples, num_features)\n",
    "\n",
    "### 훈련 데이터의 차원 확인\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "### 훈련 데이터의 벡터 값 확인\n",
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트 데이터 확인\n",
    "new_post = ['우리 과이 먹으로 갖']\n",
    "new_post_tokens = [Okt.morphs(row) for row in new_post]\n",
    "\n",
    "new_post_for_vectorize = []\n",
    "\n",
    "for content in new_post_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "        \n",
    "    new_post_for_vectorize.append(sentence)\n",
    "    \n",
    "print(new_post_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 샘플 데이터 확인\n",
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 샘플 데이터와 훈련 데이터 거리 확인\n",
    "import scipy as sp\n",
    "\n",
    "#거리 구해주는 함수\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    \n",
    "    print(\"== %i 번째 문장과의 거리:%.2f   : %s\" %(i,d,contents[i]))\n",
    "    \n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 가장 가까운 거리의 훈련 데이터 거리 확인\n",
    "print(\"가장 가까운 문장은 %i번째 이고 거리는 %.2f\" % (best_i, best_dist))\n",
    "print('-->', new_post)\n",
    "print('---->', contents[best_i])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 훈련 데이터와의 벡터 값 확인\n",
    "for i in range(0,len(contents)):\n",
    "    print(X.getrow(i).toarray())\n",
    "    \n",
    "print('---------------------')\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 군집 분석을 이용한 문장의 유사도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k 평균 군집 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 읽어오기\n",
    "import pandas as pd\n",
    "Data = pd.read_csv('./data/군집분석데이터.csv', encoding='cp949')\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "docs = []\n",
    "for i in Data['기사내용']:\n",
    "    docs.append(hannanum.nouns(i))\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 문장을 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3).fit(df)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주성분 분석을 수행한 후 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(df)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.index=Data['검색어']\n",
    "kmeans.labels_ == 0\n",
    "\n",
    "# x축 : first y축 : second 번호로 나타낸후 plot으로 시각화\n",
    "plt.scatter(principalDf.iloc[kmeans.labels_ == 0, 0], principalDf.iloc[kmeans.labels_ == 0, 1], s = 10, c = 'red', label = 'cluster1')\n",
    "plt.scatter(principalDf.iloc[kmeans.labels_ == 1, 0], principalDf.iloc[kmeans.labels_ == 1, 1], s = 10, c = 'blue', label = 'cluster2')\n",
    "plt.scatter(principalDf.iloc[kmeans.labels_ == 2, 0], principalDf.iloc[kmeans.labels_ == 2, 1], s = 10, c = 'green', label = 'cluster3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구조적 분석 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 읽어오기\n",
    "import pandas as pd\n",
    "Data = pd.read_csv('./data/군집분석데이터.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "#형태소 분석\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "docs = []\n",
    "for i in Data['기사내용']:\n",
    "    docs.append(hannanum.nouns(i))\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "    \n",
    "#각 문장을 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=3, linkage='ward')  \n",
    "cluster.fit_predict(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#덴도 그램 그리기\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Customer Dendograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(df, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob 사전을 이용한 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#감성 분석을 위한 텍스트 생성\n",
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")\n",
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "\n",
    "# 감성 출력   \n",
    "print('Sentiment of wiki: ', wiki.sentiment)\n",
    "print('Sentiment of wiki: ', testimonial.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### afinn 사전을 이용한 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)\n",
    "\n",
    "doc_a = \"i love you.\"\n",
    "doc_b = \"i hate you\"\n",
    "doc_c = \"i like valleybool\"\n",
    "\n",
    "doc_set = [doc_a, doc_b, doc_c]\n",
    "for text in doc_set:\n",
    "    print(text)\n",
    "    print('Predicted Sentiment polarity:', afn.score(text))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from afinn import Afinn\n",
    "\n",
    "pos_review=(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))[20]\n",
    "\n",
    "f = open(pos_review, 'r')\n",
    "lines1=f.readlines()[0]\n",
    "f.close()\n",
    "print(lines1)\n",
    "\n",
    "afinn = Afinn()\n",
    "print(afinn.score(lines1))\n",
    "\n",
    "neg_review=(glob.glob(\"./data/aclImdb/train/neg/*.txt\"))[20]\n",
    "f = open(neg_review, 'r')\n",
    "lines2 = f.readlines()[0]\n",
    "f.close()\n",
    "print(afinn.score(lines2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기계학습으로 감성 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀를 이용한 감성 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계학습으로 감성분석\n",
    "import pandas as pd\n",
    "import glob\n",
    "from nltk.corpus import stopwords \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#영화 긍정 문장 읽어오기\n",
    "pos_review=(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))\n",
    "# 긍정,부정 텍스트 읽어오기\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영화 부정 문장 읽어오기\n",
    "neg_review=(glob.glob(\"./data/aclImdb/train/neg/*.txt\"))\n",
    "lines_neg=[]\n",
    "for i in neg_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_neg.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "len(lines_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#긍정 부정 문장 합치기\n",
    "total_text=lines_pos+lines_neg\n",
    "len(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#긍정과 부정 인덱스 만들기\n",
    "x = np.array([\"pos\", \"neg\"])\n",
    "class_Index=np.repeat(x, [len(lines_pos), len(lines_neg)], axis=0)\n",
    "print(class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정규화\n",
    "stop_words = stopwords.words('english')\n",
    "#stopwords의 단어를 제거하고 동일하게 추출된 단어의 빈도수를 직접 이용하지 않고 가중치를 축소하기\n",
    "vect = TfidfVectorizer(stop_words=stop_words).fit(total_text)\n",
    "#훈련 데이터를 벡터화\n",
    "X_train_vectorized = vect.transform(total_text)\n",
    "#긍정, 부정 레이블을 index로 설정\n",
    "X_train_vectorized.index = class_Index\n",
    "print(X_train_vectorized[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로지스틱 회귀 분석을 이용해서 훈련 데이터를 가지고 모델을 생성\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증 데이터를 가지고 검증\n",
    "pos_review_test=(glob.glob(\"./data/aclImdb/test/pos/*.txt\"))[10]\n",
    "test=[]\n",
    "f = open(pos_review_test, 'r')\n",
    "test.append(f.readlines()[0])\n",
    "f.close()\n",
    "predictions = model.predict(vect.transform(test))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_review_test=(glob.glob(\"./data/aclImdb/test/neg/*.txt\"))[20]\n",
    "test2=[]\n",
    "f = open(neg_review_test, 'r')\n",
    "test2.append(f.readlines()[0])\n",
    "f.close()\n",
    "\n",
    "predictions = model.predict(vect.transform(test2))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 의사결정나무 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "predictions = clf.predict(vect.transform(test))\n",
    "print(predictions)\n",
    "\n",
    "predictions = clf.predict(vect.transform(test2))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나이브베이즈 감성 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어별 분류를 위한 패키지 import\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "\n",
    "### 훈련 데이터 만들기\n",
    "train = [('i like you', 'pos'), \n",
    "         ('i do not like you', 'neg'),\n",
    "         ('i hate you', 'neg'), \n",
    "         ('i do not hate you', 'pos'),\n",
    "        ('i love you', 'pos'),\n",
    "        ('I do not love you', 'neg')]\n",
    "\n",
    "#분류기 만들기\n",
    "classifier = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 정확도 확인\n",
    "test = [('i do not like jessica', 'neg'),('i like jessica', 'pos')]\n",
    "classifier.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#없는 문장으로 확인\n",
    "print(classifier.classify(\"Their burgers are amazing\"))\n",
    "print(classifier.classify(\"I don't like their pizza.\"))\n",
    "\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The beer was amazing. \"\n",
    "                \"But the hangover was horrible. My boss was not happy.\",\n",
    "                classifier=classifier)\n",
    "print(blob.classify())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 감성 분석\n",
    "train = [\n",
    "    ('나는 이 샌드위치를 정말 좋아해.', '긍정'),\n",
    "    ('정말 멋진 곳이에요!', '긍정'),\n",
    "    ('나는 이 맥주들이 아주 좋다고 생각해요.', '긍정'),\n",
    "    ('이것은 나의 최고의 작품입니다.', '긍정'),\n",
    "    (\"정말 멋진 광경이다\", \"긍정\"),\n",
    "    ('난 이 식당 싫어', '부정'),\n",
    "    ('난 이게 지겨워.', '부정'),\n",
    "    (\"이 문제는 처리할 수 없습니다.\", \"부정\"),\n",
    "    ('그는 나의 불구대천의 원수이다.', '부정'),\n",
    "    ('내 상사는 끔찍해.', '부정'),\n",
    "    ('나는 운동을 좋아하지 않습니다.', '부정'),\n",
    "    ('나는 낮잠 자는 것을 좋아하지 않습니다.', '부정'),\n",
    "    ('맛있는 것은 언제 먹어도 기분이 좋아져요', '긍정')\n",
    "]\n",
    "\n",
    "test = [\n",
    "    ('맥주가 좋았습니다.', '긍정'),\n",
    "    ('난 내 일을 즐기지 않는다', '부정'),\n",
    "    ('오늘은 기분이 안 좋아요.', '부정'),\n",
    "    ('놀라워요!', '긍정'),\n",
    "    ('네드는 나의 친구입니다.', '긍정'),\n",
    "    ('제가 이렇게 하고 있다니 믿을 수가 없어요.', '부정'),\n",
    "    ('영화보는 것을 좋아하지 않습니다..', '부정'),\n",
    "    ('토스트는 언제 먹어도 종아요..', '긍정')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석을 하지 않고 분류기 만들기\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier(train)\n",
    "classifier.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 형태소 분석기 가져오기\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "train_data = [(['/'.join(token) for token in hannanum.pos(sentence)], result) for [sentence, result] in train]\n",
    "classifier = NaiveBayesClassifier(train_data)\n",
    "classifier.show_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터로 검증\n",
    "test_data = [(['/'.join(token) for token in hannanum.pos(sentence)], result) for [sentence, result] in test]\n",
    "classifier.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#없는 문장으로 확인\n",
    "sample = \"나는 배구를 좋아합니다.\"\n",
    "print(classifier.classify(['/'.join(token) for token in hannanum.pos(sample)]))\n",
    "\n",
    "sample = \"축구하는 것을 좋아하지 않습니다.\"\n",
    "print(classifier.classify(['/'.join(token) for token in hannanum.pos(sample)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연관분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동시출현 기반 연관어 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터의 긍정 문장 100개 읽어서 lines_pos에 저장하기\n",
    "import glob\n",
    "pos_review=(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "        \n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#단어만 추출하기 위한 정규식\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "#영문 불용어 제거를 위한 불용어 사정\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "count = {}   #동시출현 빈도가 저장될 dict\n",
    "for line in lines_pos:\n",
    "    #소문자로 변환\n",
    "    words =  line.lower()\n",
    "    #단어만 추출\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    #불용어 제거 - 중복을 제거하기 위해서 set으로 변환\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+[\"br\"]]\n",
    "    #2글자 이상만 추출\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "    #문장들을 순회하면서 단어들이 같이 출현하는 빈도수를 디셔너리에 저장\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "            if a>b: \n",
    "                count[b, a] = count.get((b, a),0) + 1  \n",
    "            else :\n",
    "                count[a, b] = count.get((a, b),0) + 1  \n",
    "#디셔너리를 가지고 데이터프레임 생성\n",
    "df=pd.DataFrame.from_dict(count, orient='index')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "#인덱스를 분리하고 데이터를 가지고 list를 생성\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "#list를 이용해서 데이터프레임을 생성하고 컬럼이름 설정    \n",
    "df2=pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "#출현 빈도수로 데이터를 내림차순 정렬해서 데이터프레임을 다시 생성\n",
    "df3=df2.sort_values(by=['freq'],ascending=False)\n",
    "#인덱스를 다시 생성\n",
    "df3_pos=df3.reset_index(drop=True)\n",
    "df3_pos.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 이용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 리뷰 데이터에 적용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터의 긍정 문장 100개 읽어서 lines_pos에 저장하기\n",
    "import glob\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "pos_review=(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "        \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "\n",
    "text=[]\n",
    "for line in lines_pos:\n",
    "    words =  line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+[\"br\"]]\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "    text.append(stopped_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "#sg는 skip-gram을 적용하겠다는 옵션이고 min_count는 적어도 3번이상 나온 경우에만 적용\n",
    "model = Word2Vec(text, sg=1, window=2, min_count=3)\n",
    "model.init_sims(replace=True)\n",
    "#film 과 movie의 관계\n",
    "model.wv.similarity('film', 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"good\",topn =5)\n",
    "len(model.wv.index2word)\n",
    "print(model.wv.index2word[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec - 네이버 지식인에서 국민카드와 관련된 텍스트를 조회해서 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "from matplotlib import font_manager, rc\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "#텍스트를 저장할 리스트 생성\n",
    "kb_candi_text = []\n",
    "\n",
    "#다운로드 받을 주소 생성\n",
    "html = 'https://search.naver.com/search.naver?where=kin&sm=tab_jum&ie=utf8&query=' + urllib.parse.quote('국민카드') + '&start='\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000개의 데이터를 10개씩 0.5초마다 쉬면서 다운로드 받아서 dl 태그의 내용만 가져와서 텍스트를 저장\n",
    "for n in range(1, 1000, 10):\n",
    "    response = urllib.request.urlopen(html + str(n))\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    tmp = soup.find_all('dl')\n",
    "    for line in tmp:\n",
    "        kb_candi_text.append(line.text)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "print(kb_candi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 형태소 분석기를 이용해서 단어 단위로 분할\n",
    "import nltk\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "kb_text = ''\n",
    "\n",
    "for each_line in kb_candi_text[:10000]:\n",
    "    kb_text = kb_text + each_line + '\\n'\n",
    "tokens_ko = Okt.morphs(kb_text)\n",
    "print(tokens_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='국민카드')\n",
    "print(ko.vocab().most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['.','가','요','답변','...','을','수','에','질문','제','를','이','도',\n",
    "                      '좋','1','는','로','으로','2','것','은','다',',','니다','대','들',\n",
    "                      '2017','들','데','..','의','때','겠','고','게','네요','한','일','할',\n",
    "                      '10','?','하는','06','주','려고','인데','거','좀','는데','~','ㅎㅎ',\n",
    "                      '하나','이상','20','뭐','까','있는','잘','습니다','다면','했','주려',\n",
    "                      '지','있','못','후','중','줄','6','과','어떤','기본','!!',\n",
    "                      '단어','라고','중요한','합','가요','....','보이','네','무지']\n",
    "#불용어 제거\n",
    "tokens_ko = [each_word for each_word in tokens_ko \n",
    "    if each_word not in stop_words]\n",
    "\n",
    "#가장 많이 등장한 50개 단어 출력\n",
    "ko = nltk.Text(tokens_ko, name='국민카드')\n",
    "ko.vocab().most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ko.plot(50) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "data = ko.vocab().most_common(300)\n",
    "\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.5,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연관어 분석\n",
    "from gensim.models import word2vec\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "results = []\n",
    "lines = kb_candi_text\n",
    "\n",
    "#품사를 확인해서 조사나 어미나 문장부호 제거\n",
    "for line in lines:\n",
    "    malist = Okt.pos(line, norm=True, stem=True)\n",
    "    r = []\n",
    "\n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "\n",
    "    r1 = (\" \".join(r)).strip()\n",
    "    results.append(r1)\n",
    "    print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'kb.data'\n",
    "with open(data_file, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(results))\n",
    "    data = word2vec.LineSentence(data_file)\n",
    "model = word2vec.Word2Vec(data, size=200, window=10, hs=1,min_count=2, sg=1)\n",
    "model.save('kb.model')\n",
    "model = word2vec.Word2Vec.load(\"kb.model\")\n",
    "model.most_similar(positive=['국민카드'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['할인'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apriori 알고리즘 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "#데이터 생성\n",
    "dataset=[['사과','치즈','생수'],\n",
    "['생수','호두','치즈','고등어'],\n",
    "['수박','사과','생수'],\n",
    "['생수','호두','치즈','옥수수']]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지지도를 0.5로 설정\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구매항목 연관분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/구매내역.csv', encoding='cp949')\n",
    "customer = df['고객명'].unique()\n",
    "print(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for i in customer:\n",
    "    temp = df[df['고객명'] == i]\n",
    "    item = []\n",
    "    \n",
    "    for imsi in temp['구매항목']:\n",
    "        item.append(imsi)\n",
    "    items.append(item)\n",
    "    \n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(items).transform(items)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지지도를 0.3로 설정\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연관분석 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터의 긍정 문장 100개 읽어서 lines_pos에 저장하기\n",
    "import glob\n",
    "pos_review=(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#단어만 추출하기 위한 정규식\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "#영문 불용어 제거를 위한 불용어 사전\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "count = {}   #동시출현 빈도가 저장될 dict\n",
    "for line in lines_pos:\n",
    "    #소문자로 변환\n",
    "    words =  line.lower()\n",
    "    #단어만 추출\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    #불용어 제거 - 중복을 제거하기 위해서 set으로 변환\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+[\"br\"]]\n",
    "    #2글자 이상만 추출\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "\n",
    "#문장들을 순회하면 단어들이 같이 출현하는 빈도수를 디셔너리에 저장\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "            if a>b: \n",
    "                count[b, a] = count.get((b, a),0) + 1  \n",
    "            else :\n",
    "                count[a, b] = count.get((a, b),0) + 1  \n",
    "#디셔너리를 가지고 데이터프레임 생성\n",
    "df=pd.DataFrame.from_dict(count, orient='index')\n",
    "\n",
    "list1=[]\n",
    "#인덱스를 분리하고 데이터를 가지고 list를 생성\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "#list를 이용해서 데이터프레임을 생성하고 컬럼이름 설정    \n",
    "df2=pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "#출현 빈도수로 데이터를 내림차순 정렬해서 데이터프레임을 다시 생성\n",
    "df3=df2.sort_values(by=['freq'],ascending=False)\n",
    "#인덱스를 다시 생성\n",
    "df3_pos=df3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "G_pos=nx.Graph()\n",
    "for i in range((len(np.where(df3_pos['freq']>10)[0]))):\n",
    "    G_pos.add_edge(df3_pos['term1'][i], df3_pos['term2'][i], weight=int(df3_pos['freq'][i]))\n",
    "dgr = nx.degree_centrality(G_pos)\n",
    "btw = nx.betweenness_centrality(G_pos)\n",
    "cls = nx.closeness_centrality(G_pos)\n",
    "egv = nx.eigenvector_centrality(G_pos)\n",
    "\n",
    "sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "G = nx.Graph()\n",
    "for i in range(len(sorted_cls)):\n",
    "    G.add_node(sorted_cls[i][0], nodesize=sorted_dgr[i][1])\n",
    "\n",
    "for i in range((len(np.where(df3_pos['freq']>10)[0]))):\n",
    "    G.add_weighted_edges_from([(df3_pos['term1'][i], df3_pos['term2'][i],int(df3_pos['freq'][i]))])\n",
    "\n",
    "sizes = [G.node[node]['nodesize']*500 for node in G]\n",
    "\n",
    "options = {\n",
    "    'edge_color': '#FFDEA2',\n",
    "    'width': 1,\n",
    "    'with_labels': True,\n",
    "    'font_weight': 'regular',\n",
    "}\n",
    "\n",
    "nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=3.5, iterations=50), **options)\n",
    "ax = plt.gca()\n",
    "ax.collections[0].set_edgecolor(\"#555555\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}